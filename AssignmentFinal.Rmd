---
title: "CSML1000-003-O-F19 - Group 2 - Assignment Final"
author: "Rajiv Kaushik, Yuming Cui, Madana Bolla, Pratik Chandwani, Konstantin Krassavine"
date: "09/11/2019"
output: html_document
---

# Data Description

##Context
This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The "goal" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4.

##Content

Attribute Information:
> 1. age
> 2. sex
> 3. chest pain type (4 values)
> 4. resting blood pressure
> 5. serum cholestoral in mg/dl
> 6. fasting blood sugar > 120 mg/dl
> 7. resting electrocardiographic results (values 0,1,2)
> 8. maximum heart rate achieved
> 9. exercise induced angina
> 10. oldpeak = ST depression induced by exercise relative to rest
> 11. the slope of the peak exercise ST segment
> 12. number of major vessels (0-3) colored by flourosopy
> 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect
The names and social security numbers of the patients were recently removed from the database, replaced with dummy values. One file has been "processed", that one containing the Cleveland database. All four unprocessed files also exist in this directory.


#Install packages
```{r echo=FALSE message = FALSE}
#install.packages("mice")
#install.packages("VIM")
#install.packages("Hmisc")
#install.packages("knitr", dependencies = TRUE)
library(Hmisc)
library(caret)
library(mice)
library(VIM)
library(ggplot2)
library(EnvStats)
library(cluster)
library(corrplot)
library(NbClust)
library(factoextra)
library(knitr)
library(pROC)
library(rpart)
library(rpart.plot)
library(neuralnet)
library(gbm)
library(ROCR)
library(effects)
```

## Load dataset
Read data
```{r}
#Read data
heart <- read.csv('./data/heart-with-na.csv', header=TRUE, sep = ",")
```

View summary of data
```{r}
head(heart, 5)
summary(heart)
```

## Data imputation preparation
### View miss data pattern with md.pattern
```{r}
md.pattern(heart)
```
### View miss data pattern with VIM
```{r}
 mice_plot <- aggr(heart, col=c('navyblue','yellow'),
                    numbers=TRUE, sortVars=TRUE,
                    labels=names(heart), cex.axis=.7,
                    gap=3, ylab=c("Missing data","Pattern"))
```
Let's quickly understand this, There are 64% values in the data set with no missing values. There are 15% missing values in age, 10% in fbs, 10% in slope, and 9% in cp.

### Impute missing values
m  – Refers to 5 imputed data sets
maxit – Refers to no. of iterations taken to impute missing values
method – Refers to method used in imputation. we used predictive mean matching.
```{r message=FALSE, echo=FALSE,warning=FALSE}
imputed_Data <- mice(heart, m=5, maxit = 10, method = 'pmm', seed = 500)
imputed_Data
```

Check imputed age vlaues:
```{r}
imputed_Data$imp$age
```
Check imputed cp(chect pain scale) vlaues:
```{r}
imputed_Data$imp$cp
```

Merge the imputed values back to the dataset.
```{r}
combineData<-complete(imputed_Data)
summary(combineData)
```

Now combineData contains all the imputed values and non missing values.

# summary(raw)

## Visualizations
```{r}
#Melt data
melt_data = melt(combineData, id.vars=c("X"))
#visualize spread of data
ggplot(melt_data,  mapping = aes(x = value)) + geom_bar(fill = "#FF6666") + facet_wrap(~variable, scales = 'free_x')
boxplot(combineData[,-c(1)])
```
Above shows data needs scaling and has outliers

## Checking outlier
```{r}
#3 outlier
rosnerTest(combineData$age, k = 4, warn = F)
#3 outlier
rosnerTest(combineData$cp, k = 4, warn = F)
#3 outlier
rosnerTest(combineData$trestbps, k = 4, warn = F)
#0 outliers
rosnerTest(combineData$chol, k = 4, warn = F)
#4 outliers
rosnerTest(combineData$fbs, k = 4, warn = F)
#4 outliers
rosnerTest(combineData$restecg, k = 4, warn = F)
#4 outliers
rosnerTest(combineData$thalach, k = 4, warn = F)
#4 outliers
rosnerTest(combineData$exang, k = 4, warn = F)
#4 outliers
rosnerTest(combineData$oldpeak, k = 4, warn = F)
#4 outliers
rosnerTest(combineData$slope, k = 4, warn = F)
#0 outliers
rosnerTest(combineData$ca, k = 4, warn = F)
#2 outliers
rosnerTest(combineData$thal, k = 4, warn = F)
```
*Only these four columns have outliers
+ trestbps
+ chol
+ oldpeak
+ thal

# Data preparation
We replaced outliers with 5th and 95th percentile values.

```{r}
#replace outliers with 5th and 95th percentile values
#remember An outlier is not any point over the 95th percentile 
#or below the 5th percentile. Instead, an outlier is considered so 
#if it is below the first quartile – 1.5·IQR or above third quartile + 1.5·IQR.
capOutlier <- function(x){
   qnt <- quantile(x, probs=c(.25, .75), na.rm = T)
   caps <- quantile(x, probs=c(.05, .95), na.rm = T)
   H <- 1.5 * IQR(x, na.rm = T)
   x[x < (qnt[1] - H)] <- caps[1]
   x[x > (qnt[2] + H)] <- caps[2]
   return(x)
}
combineData$trestbps=capOutlier(combineData$trestbps)
combineData$chol=capOutlier(combineData$chol)
combineData$oldpeak=capOutlier(combineData$oldpeak)
combineData$thal=capOutlier(combineData$thal)



#scale data, and remove rowid and target
data_scaled <- data.frame(scale(combineData[-c(1, 15)]))
#visualize how well data was scaled
boxplot(data_scaled)
#scaling looks good
```



```{r}
#corr of data
corrmatrix <- cor(data_scaled)
corrplot(corrmatrix, method = 'pie', type="upper")
```
There is no two columns have strong correlations bigger than 50%



# PCA and reduction

```{r}
# ##### Build pca using princomp
data_pca1 <- princomp(data_scaled)
#examine the importance of PCs
summary(data_pca1)
# inspect principal components
# loadings shows variance and and how much each variable contributes to each components
loadings(data_pca1)
fviz_pca_var(data_pca1,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

Above shows that first 10 give us 89% of the variance
We don't see one variable being overbearing


```{r}
# ###### using princomp first 10 give us 90% of the variance
# ##### build pca using prcomp
data_pca2 <- prcomp(data_scaled)
summary(data_pca2)
#above shows first 6 account for 88% variance
#plot
plot(data_pca2)
# scree plot
plot(data_pca2, type = "lines")

fviz_pca_var(data_pca2,
             col.var = "contrib", # Color by contributions
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```
Above shows first 10 account for 89% variance

Please note: The result from PCA tells us there is no varaiables which dominate, therefore we are not able to reduce any demisions.  

## Exploration of the data

Drop row id
```{r}
data<- combineData[-c(1)]
```

We explored the different variables in combineData to see which ones were more likes to affect our random forest algorithm.
```{r}
names(data)
```
```{r}
library(ggplot2)
```

Looks like male has better change to get heart desease
```{r echo=FALSE}
plot(factor(target) ~ sex, data = data)
title(main="Target by Gender")
```


The higher cholestoral is not necessary the higher change of getting heart desease.
```{r echo=FALSE}
ggplot(data=data, aes(chol, target)) + geom_jitter(height=0.03, alpha=0.2) + stat_smooth(method="loess", alpha=0.2, col="red") + ggtitle("Target By Serum cholestoral in mg/dl") + theme_bw()
```

And blood presssure is not playing a signicent role in heart deasease
```{r echo=FALSE}
ggplot(data=data, aes(trestbps, target)) + geom_jitter(height=0.03, alpha=0.2) + stat_smooth(method="loess", alpha=0.2, col="red") + ggtitle("Target By Resting blood pressure") + theme_bw()
```

If you heart beat too fast when you are resting, you should be more careful.
```{r echo=FALSE}
ggplot(data=data, aes(thalach, target)) + geom_jitter(height=0.03, alpha=0.2) + stat_smooth(method="loess", alpha=0.2, col="red") + ggtitle("Target By heart rate ") + theme_bw()
```

```{r}
names(data)
```

```{r}
library(caret)
set.seed(3456)
trainingIndex = createDataPartition(data$age, p = 0.75, list=FALSE)
trainData = data[trainingIndex,]
testData = data[-trainingIndex,]
nrow(trainData)
```


##Logistic Regression Model
```{r}
lr.fit <- glm(target~ age+sex+cp+trestbps+chol+fbs+restecg+thalach+exang+oldpeak+slope+ca, data=trainData, family = binomial)
###Shows that the following variables are predictors of target variable
###sex,cp, restecg, exang, oldpeak, ca
summary(lr.fit)
lr.prob=predict(lr.fit,type="response")
summary(lr.prob)
#compute average prediction for true outcomes
#TP are 78%, that is predicting presence of heart disease correctly 78% of the time 
#TN are 26%, that is predicting no heart disease when there is none 26% of the time
#78% is not bad, it can be improved
tapply(lr.prob,trainData$target,mean)
#now we will convert probabilities to predictions using ROC curves
#ROC curve will help find the threshold
lr.predict = prediction(lr.prob, trainData$target)
lr.perf = performance(lr.predict, "tpr", "fpr")
# Plot ROC curve
plot(lr.perf)
# Add colors
plot(lr.perf, colorize=TRUE)
# Add threshold labels 
plot(lr.perf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
#We want high TRUE POSITIVES or Sensitivity for diagnosing heart disease, and are ok with higher false positive 
#Therefore the threshold is at (0.9,0.2) where 90% with heart disease are diagnosed correctly
#At this the threshold is 0.5. This implies that a probability above 0.5 should be classified as heart disease 


#with 0.5 as threshold, lets predict on test dataset
lrtest.prob=predict(lr.fit,type="response", newdata=testData)
#generate confusion matrix 
#Confusion matris shows that model predicts no heart disease for 4 people who have heart disease
#Also predicts heart disease correctly for 35 peoplewho actually have heart disease
table(testData$target,lrtest.prob >= 0.5)
sensitivity = (35/(35+4))*100
# Logistic Regression Model Sensitivity= 89%
# In order to assess the performance of our model, we will delineate the ROC curve. ROC is also known as Receiver Optimistic Characteristics
# ROC curve validates that sensitivity is 89.6% 
auc.lr = roc(testData$target, lrtest.prob, plot = TRUE, col = "blue")
auc.lr


###plot the logistic regression model against predictor variables
# After we have summarised our model, we will visualize it through the following plots
plot(allEffects(lr.fit))


```


# Now use Decision Tree algorithm and plot using rpart
```{r}
dt.fit <- rpart(target~ age+sex+cp+trestbps+chol+fbs+restecg+thalach+exang+oldpeak+slope+ca, trainData,method = 'class')
dt.predict <- predict(dt.fit, testData, type = 'class')
dt.probability <- predict(dt.fit, testData, type = 'prob')

rpart.plot(dt.fit)
rpart.plot(dt.fit)
# In order to assess the performance of our model, we will delineate the ROC curve. ROC is also known as Receiver Optimistic Characteristics
# ROC curve shows sensitivity = 82% 
auc.dt = roc(testData$target, factor(dt.predict, ordered = TRUE), plot = TRUE, col = "blue")
auc.dt


```

# Now use Artificial Neural Networks
#since predicted variable is probabilty and continous, convert to  0 or 1 factor
```{r}
ann.fit =neuralnet(target~ age+sex+cp+trestbps+chol+fbs+restecg+thalach+exang+oldpeak+slope+ca,trainData,linear.output=FALSE)
plot(ann.fit)
ann.predict=compute(ann.fit,testData)
annResult=ann.predict$net.result
annResult=ifelse(annResult>0.5,1,0)
auc.ann = roc(testData$target, factor(annResult, ordered = TRUE), plot = TRUE, col = "blue")
auc.ann
```

## Now use Gradient Boosting. This uses ensemble models like weak decision trees
##These decision trees combine together to form a strong model of gradient boosting
```{r}
system.time(
gbm.fit <- gbm(target~ age+sex+cp+trestbps+chol+fbs+restecg+thalach+exang+oldpeak+slope+ca
, distribution = "bernoulli"
, data =trainData
, n.trees = 1000
, interaction.depth = 6
, n.minobsinnode = 10
, shrinkage = 0.1
, bag.fraction = 0.5
, train.fraction = nrow(trainData) / (nrow(trainData) + nrow(testData))
)
)
# Determine best iteration based on test data
gbm.iter = gbm.perf(gbm.fit, method = "test")
model.influence = relative.influence(gbm.fit, n.trees = gbm.iter, sort. = TRUE)
#Plot the gbm model
plot(gbm.fit)
# Plot and calculate AUC on test data
gbm.test = predict(gbm.fit, newdata = testData, n.trees = gbm.iter)
auc.gbm = roc(testData$target, gbm.test, plot = TRUE, col = "red")
print(auc.gbm)
auc.gbm
```

