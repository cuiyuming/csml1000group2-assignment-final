---
title: "CSML1000-003-O-F19 - Group 2 Project"
author: "Rajiv Kaushik, Yuming Cui, Madana Bolla, Pratik Chandwani, Konstantin Krassavine"
date: "11/15/2019"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE,warning=FALSE,message=FALSE,include=TRUE)
```

image: ![](./images/heartdisease.jpg)


# CRISP-DM Modeling Heart Disease on Subjects in Cleveland, OH, USA

Kaggle Repository for this dataset is located at [here](https://www.kaggle.com/ronitf/heart-disease-uci) 


# Business Understanding


### Problem Statement

Predict heart disease in a patient from Cleveland Medical Center, OH, USA. Since this analysis and modeling covers risk to human life, it is important to maintain a high rate of identifying patients with heart disease (i.e True Positives, therefore high sensitivity) while maintaining a realistic error rate for those who do not have heart disease ( False Positives, redicting heart disease in a patient when there is none) 


### Context

This database contains data about the factors related to heart disease. There are 14 attributes from Cleveland Clinic Foundation. 
The "target" field refers to the presence or high risk of heart disease in the subject. 

* 0 represents no heart disease present
* 1 implies presence of heart disease"  


### Content

Attribute Information:

 1. age
 2. sex
 3. chest pain type (4 values)
 4. resting blood pressure
 5. serum cholestoral in mg/dl
 6. fasting blood sugar > 120 mg/dl
 7. resting electrocardiographic results (values 0,1,2)
 8. maximum heart rate achieved
 9. exercise induced angina
 10. oldpeak = ST depression induced by exercise relative to rest
 11. the slope of the peak exercise ST segment
 12. number of major vessels (0-3) colored by flourosopy
 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect


### Solution Approach
1. Supervised Classification
2. Apply 4 models- Logistic Regression, Decision Tree, Artificial Neural Network, Gradient Boosting
3. Since prediction of target variable is a probability, use the models to figure out the probability threshold to delineate between no heart disease versus presence of heart disease 


```{r }
library(Hmisc)
library(caret)
library(mice)
library(VIM)
library(ggplot2)
library(EnvStats)
library(cluster)
library(corrplot)
library(NbClust)
library(factoextra)
library(knitr)
library(pROC)
library(rpart)
library(rpart.plot)
library(neuralnet)
library(gbm)
library(ROCR)
library(effects)
library(dplyr)
```

```{r}
## Load dataset
raw <- read.csv('./data/heart-with-na.csv', header=TRUE, sep = ",")
```


# Data Understanding


### Sample rows

```{r}

head(raw, 5)

```

### Statistical summary of dataset

```{r}
summary(raw)
```

### Summary of overall raw dataset
1. 55% of subjects have heart disease
2. 68% are males 
3. 44% males have heart disease
4. 75% females have heart disease

```{r}
#dim(raw)
prop.table(table(raw$target))
prop.table(table(raw$sex))
males <- raw %>% filter(sex == 1)
prop.table(table(males$target))
females <- raw %>% filter(sex == 0)
prop.table(table(females$target))

```


### Summary of missing data

There are 64% values in the data set with no missing values. There are 11% missing values in age, 6% in fbs, 5% in slope, and 5% in cp.


```{r}
### View miss data pattern with VIM

 mice_plot <- aggr(raw, col=c('navyblue','yellow'),
                    numbers=TRUE, sortVars=TRUE,
                    labels=names(raw), cex.axis=.7,
                    gap=3, ylab=c("Missing data","Pattern"))
```



### Data Imputation

**Missing data is imputed using predictive mean matching with Mice. **
Predictive Mean Matching (PMM) is a semi-parametric imputation approach. It is similar to the regression method except that for each missing value, it fills in a value randomly from among the a observed donor values from an observation whose regression-predicted values are closest to the regression-predicted value for the missing value from the simulated regression model (Heitjan and Little 1991; Schenker and Taylor 1996).
The PMM method ensures that imputed values are plausible; it might be more appropriate than the regression method (which assumes a joint multivariate normal distribution) if the normality assumption is violated (Horton and Lipsitz 2001, p. 246).

```{r include=FALSE}

# Impute missing values
# m  – Refers to number of imputations. Typically 5 is used. New rule of thumb to use whatever the average percentage rate of missingness is - so if there is 30% missing data on average in a dataset, use 30 imputations - see Bodner (2008) and White et al (2011) for further details.
# maxit – Refers to no. of iterations taken to impute missing values
# method – Refers to method used in imputation. we used predictive mean matching.


imputed <- mice(raw, m=5, maxit = 10, method = 'pmm', seed = 500)
#imputed
#summary(imputed)
```

**Missing data after imputation**.

Dataset is now complete as shown below.


```{r}

# combine data back into main dataset
heart<-complete(imputed)

# Save dataset heart for shiny app.
save(heart, file = "./HeartDiseaseCleveland/heart.RData")
#summary(heart)

 mice_plot <- aggr(heart, col=c('navyblue','yellow'),
                    numbers=TRUE, sortVars=TRUE,
                    labels=names(heart), cex.axis=.7,
                    gap=3, ylab=c("Missing data","Pattern"),main="Missing Data View after Imputation")


```



### Data Exploration


**Correlation Matrix**

Visualize with a correlation matrix how features are correlated with each other and with the target variable. **No two features have a strong correlation (<50%).**

```{r}
#corr of data
corrmatrix <- cor(heart[,c(-1)])
corrplot(corrmatrix, method = 'number', type="upper")
```



**Pairwise Correlations**

1. **Gender**

* **Heart disease is more prevalent in females than males**
* **45% males have heart disease**
* **75% females have heart disease**

```{r echo=FALSE}

males <- heart %>% filter(sex == 1)
prop.table(table(males$target))
females <- heart %>% filter(sex == 0)
prop.table(table(females$target))

ggplot(heart, aes(x=sex, fill=as.factor(target))) +
  geom_bar() +
   ggtitle("Target By Gender")


```

2. **Age**

**Rate of heart disease increases with age over 60**

```{r echo=FALSE}
ggplot(data=heart, aes(age, target)) + geom_jitter(height=0.03, alpha=0.2) + stat_smooth(method="loess", alpha=0.2, col="red") + ggtitle("Target By Age") + theme_bw()
```


3. **Cholesterol**

**Higher cholesterol increases rate of heart disease.** We don't know if this is good (HDL) or bad (LDL) cholesterol or total cholesterol

```{r echo=FALSE}
ggplot(data=heart, aes(chol, target)) + geom_jitter(height=0.03, alpha=0.2) + stat_smooth(method="loess", alpha=0.2, col="red") + ggtitle("Target By Serum cholestoral in mg/dl") + theme_bw()
```


4. **Fasting Blood Sugar**

**Of those subjects with fasting blood sugar > 120 mg/dl, majority have higher risk of heart disease **

```{r echo=FALSE}


ggplot(heart, aes(x=fbs, fill=as.factor(target))) +
  geom_bar() +
   ggtitle("Target By Fasting Sugar above or below 120 mg/dl")

```

5. **Resting ECG results**

**Subjects with higher resting ECG have a higher prevalence of heart disease** 

```{r echo=FALSE}

ggplot(heart, aes(x=restecg, fill=as.factor(target))) +
  geom_bar() +
   ggtitle("Target By levels of Resting Heart Rate")

```




# Data Preparation


### Visualization of scale and outliers

We use the Rosner Tests to identify outlier. 

**Outcomes are:**

1. Needs scaling
2. Four columns have outliers - resting BP (trestbps), Cholesterol (chol), Stress Test depression (oldpeak), Defects (thal)


```{r}
#Melt data
melt_data = melt(heart, id.vars=c("X"))
#visualize spread of data
ggplot(melt_data,  mapping = aes(x = value)) + geom_bar(fill = "#FF6666") + facet_wrap(~variable, scales = 'free_x') + ggtitle("Distribution of variables")
boxplot(heart,main="Outliers and Scaling of Variables")
```

```{r}
#Drop row id
heart<- heart[-c(1)]
```


```{r include=FALSE}
## Checking outlier
#3 outlier
rosnerTest(heart$age, k = 4, warn = F)
#3 outlier
rosnerTest(heart$cp, k = 4, warn = F)
#3 outlier
rosnerTest(heart$trestbps, k = 4, warn = F)
#0 outliers
rosnerTest(heart$chol, k = 4, warn = F)
#4 outliers
rosnerTest(heart$fbs, k = 4, warn = F)
#4 outliers
rosnerTest(heart$restecg, k = 4, warn = F)
#4 outliers
rosnerTest(heart$thalach, k = 4, warn = F)
#4 outliers
rosnerTest(heart$exang, k = 4, warn = F)
#4 outliers
rosnerTest(heart$oldpeak, k = 4, warn = F)
#4 outliers
rosnerTest(heart$slope, k = 4, warn = F)
#0 outliers
rosnerTest(heart$ca, k = 4, warn = F)
#2 outliers
rosnerTest(heart$thal, k = 4, warn = F)
```

### Standardize data and outlier handling/processing
1. Replaced outliers with 5th and 95th percentile values
2. Scaled data

```{r}
#replace outliers with 5th and 95th percentile values
#remember An outlier is not any point over the 95th percentile 
#or below the 5th percentile. Instead, an outlier is considered so 
#if it is below the first quartile – 1.5·IQR or above third quartile + 1.5·IQR.
capOutlier <- function(x){
   qnt <- quantile(x, probs=c(.25, .75), na.rm = T)
   caps <- quantile(x, probs=c(.05, .95), na.rm = T)
   H <- 1.5 * IQR(x, na.rm = T)
   x[x < (qnt[1] - H)] <- caps[1]
   x[x > (qnt[2] + H)] <- caps[2]
   return(x)
}
heart$trestbps=capOutlier(heart$trestbps)
heart$chol=capOutlier(heart$chol)
heart$oldpeak=capOutlier(heart$oldpeak)
heart$thal=capOutlier(heart$thal)

```

```{r}
data <-heart
toscaledata=heart[,c(1,4,5,8,10)]
#scale data, and remove target
scaled <- data.frame(scale(toscaledata))
#merge data set
data$age <- scaled$age
data$trestbps <- scaled$trestbps
data$chol <- scaled$chol
data$thalach <- scaled$thalach
data$oldpeak <- scaled$oldpeak
save(data, file = "./HeartDiseaseCleveland/scaledData.RData")
```

**Visualize prepared data**

* Data is now scaled
* Outliers have been replaced/processed

```{r}

boxplot(data,main="Standardized variables with No Outliers")

```



### PCA and Reduction of features

Let's attempt to reduce dimensionality of dataset with Principal Component Analysis (PCA).

We use two methods:
1. Spectral decomposition which examines the covariances / correlations between variables (princomp)
2. Singular value decomposition which examines the covariances / correlations between individuals (prcomp)
While both methods can easily be performed within R, the singular value decomposition method (i.e., Q-mode) is the preferred analysis for numerical accuracy (R Development Core Team 2011).

* **Both show that first 7-12 features give us 80%-90% of the variance, and that no one variable is overbearing.**
* **Due to the high number of principal components relative to input features, we are not reducing any dimensions using PCA** 


* **PRINCOMP: Notice that first 7 components represent 85% of the variance**

```{r}

# ##### Build pca using princomp
data_pca1 <- princomp(data)
#examine the importance of PCs
summary(data_pca1)
# inspect principal components
# loadings shows variance and and how much each variable contributes to each components
#loadings(data_pca1)
fviz_pca_var(data_pca1,
            col.var = "contrib", # Color by contributions to the PC
            gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
            repel = TRUE     # Avoid text overlapping
            )

```


**PRCOMP: Notice that first 12 components represent 86% of the variance**

```{r}

# ###### using princomp first 7 give us 85% of the variance
# ##### build pca using prcomp
data_pca2 <- prcomp(data)
summary(data_pca2)
#above shows first 6 account for 88% variance
#plot
#plot(data_pca2)
# scree plot
#plot(data_pca2, type = "lines")

fviz_pca_var(data_pca2,
            col.var = "contrib", # Color by contributions
            gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
            repel = TRUE     # Avoid text overlapping
            )

```



### Partition dataset
1. Training - 75%, 229 subjects
2. Testing - 25%, 74 subjects

```{r}
library(caret)
set.seed(3456)
trainingIndex = createDataPartition(data$age, p = 0.75, list=FALSE)
trainData = data[trainingIndex,]
testData = data[-trainingIndex,]

#Save train data for shiny app
save(trainData, file = "./HeartDiseaseCleveland/trainData.RData")


```


# Modeling 


### Logistic Regression Model

1. **Overview**
Logistic Regression is used when the dependent variable(target) is categorical, as in this case.

Types of Logistic Regressio
-Binary Logistic Regression
The categorical response has only two 2 possible outcomes. Example: Spam or Not
-Multinomial Logistic Regression
Three or more categories without ordering. Example: Predicting which food is preferred more (Veg, Non-Veg, Vegan)
-Ordinal Logistic Regression
Three or more categories with ordering. Example: Movie rating from 1 to 5

**p value** is an important evaluation metric.p-value helps you to decide whether there is a relationship between two variables or not.

The smaller the p-value this mean the more confident you are about the existence of relationship between the two variables. The origins of p-values comes form hypothesis testing in statistics. In hypothesis testing, you have two hypothesis:
H0 (called the null hypothesis ) : There is no relationship between the two variables.
H1 (called the alternative hypothesis): There exist a relationship between the two variables.

If the p-value is less than small threshold (often 0.05 is used), then you can reject the null hypothesis H0, which means that you decide that there is a relationship between the two variables.


2. **Run the model against training data using the Binomial algo**

Looking at the p values the **following variables are predictors of target variable**

* Gender(sex)
* Type of Chest Pain (cp)
* Resting ECG result (restecg)
* Exercise induced Angina (exang)
* Stress Test depression (oldpeak)
* Number of colored veins (ca)


3. **Evaluate and refine the model** - 

   **Compute average prediction for true outcomes**
   **TP are 78%, that is predicting presence of heart disease correctly 78% of the time**
   **TN are 26%, that is predicting no heart disease when there is none 26% of the time**


```{r}
lr.fit <- glm(target~ age+sex+cp+trestbps+chol+fbs+restecg+thalach+exang+oldpeak+slope+ca, data=trainData, family = binomial)

#Save model for shiny app
saveRDS(lr.fit, "./HeartDiseaseCleveland/logisticRegression.rds")

###Shows that the following variables are predictors of target variable
###sex,cp, restecg, exang, oldpeak, ca
summary(lr.fit)
lr.prob=predict(lr.fit,type="response")
summary(lr.prob)
#compute average prediction for true outcomes
#TP are 78%, that is predicting presence of heart disease correctly 78% of the time 
#TN are 26%, that is predicting no heart disease when there is none 26% of the time
#78% is not bad, it can be improved
tapply(lr.prob,trainData$target,mean)

```


4. **Find the threshold probability to delineate between heart disease= 0 or 1**

* ROC curve will help find the threshold
   + We want high TRUE POSITIVES or Sensitivity for diagnosing heart disease, and are ok with higher false positive 
   + Therefore the threshold is at (0.9,0.2) where 90% with heart disease are diagnosed correctly
   + **At this the threshold is 0.5. This implies that a probability above 0.5 should be classified as heart disease **

```{r}

#ROC curve will help find the threshold

#now we will convert probabilities to predictions using ROC curves
lr.predict = ROCR::prediction(lr.prob, trainData$target)
lr.perf = performance(lr.predict, "tpr", "fpr")
# Plot ROC curve
# Add threshold labels 
plot(lr.perf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7),main="ROC for Logistic Regression Model")
#We want high TRUE POSITIVES or Sensitivity for diagnosing heart disease, and are ok with higher false positive 
#Therefore the threshold is at (0.9,0.2) where 90% with heart disease are diagnosed correctly
#At this the threshold is 0.5. This implies that a probability above 0.5 should be classified as heart disease 

```

5. **Evaluate model by predicting on test dataset and generating the ROC curve**

* With 0.5 as threshold, **prediction on test dataset leads to accuracy of 89%**

The accuracy of the test depends on how well the test separates the group being tested into those with and without the disease in question. Accuracy is measured by the area under the ROC curve. An area of 1 represents a perfect test; an area of .5 represents a worthless test. A rough guide for classifying the accuracy of a diagnostic test is the traditional academic point system:

.90-1 = excellent (A)
.80-.90 = good (B)
.70-.80 = fair (C)
.60-.70 = poor (D)
.50-.60 = fail (F)



```{r}
#with 0.5 as threshold, lets predict on test dataset
lrtest.prob=predict(lr.fit,type="response", newdata=testData)
#generate confusion matrix 
#Confusion matris shows that model predicts no heart disease for 4 people who have heart disease
#Also predicts heart disease correctly for 35 peoplewho actually have heart disease
table(testData$target,lrtest.prob >= 0.5)
sensitivity = (35/(35+4))*100
# Logistic Regression Model Sensitivity= 89%
# In order to assess the performance of our model, we will delineate the ROC curve. ROC is also known as Receiver Optimistic Characteristics
# ROC curve validates that sensitivity is 89.6% 
auc.lr = roc(testData$target, lrtest.prob, plot = TRUE, col = "blue")
auc.lr
```

6. **Plot the logistic regression model against predictor variables**
   * Gender(sex)
   * Type of Chest Pain (cp)
   * Resting Heart Rate (restecg)
   * Exercise induced Angina (exang)
   * Stress Test depression (oldpeak)
   * Number of colored vessels (ca)

7. **Gender** versus Target variable

* **Heart disease is more prevalent in females than males**

```{r echo=FALSE}

ggplot(data=data, aes(x=sex, fill=as.factor(target))) +
  geom_bar() +
   ggtitle("Target By Gender")

```


8. **Type of Chest Pain** versus Target variable

* **Heart disease is predicted by types 1,2 and 3 of chest pain**

```{r echo=FALSE}

ggplot(data=data, aes(x=cp, fill=as.factor(target))) +
  geom_bar() +
   ggtitle("Target By Type of Chest Pain")

```

9. **Resting ECG result** versus Target variable

**Higher ECG result predics heart disease** 

```{r echo=FALSE}
ggplot(data=data, aes(x=cp, fill=as.factor(target))) +
  geom_bar() +
   ggtitle("Target By ECG result")
```


10. **Exercise Induced Angina** versus Target variable

**Higher exercise induced angina predicts heart disease** 

```{r echo=FALSE}
ggplot(data=data, aes(x=cp, fill=as.factor(target))) +
  geom_bar() +
   ggtitle("Target By exercise induced angina")
```


10. **Stress Test Depression** versus Target variable

**Lower depression during stress test predicts heart disease** 

```{r echo=FALSE}
ggplot(data=data, aes(oldpeak, target)) + geom_jitter(height=0.03, alpha=0.2) + stat_smooth(method="loess", alpha=0.2, col="red") + ggtitle("Target By Stress Test Depression") + theme_bw()
```



11. **Number of Colored Vessels** versus Target variable

* **Heart disease is predicted by lower number of colored vessels**

```{r echo=FALSE}

ggplot(data=data, aes(x=ca, fill=as.factor(target))) +
  geom_bar() +
   ggtitle("Target By Number of Colored Vessels")

```




### Decision Tree algorithm

1. **Overview**
A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner call recursive partitioning. This flowchart-like structure helps you in decision making. It's visualization like a flowchart diagram which easily mimics the human level thinking. That is why decision trees are easy to understand and interpret.


2. **Run the model against training data, predict target on test data**

```{r}
dt.fit <- rpart(target~ age+sex+cp+trestbps+chol+fbs+restecg+thalach+exang+oldpeak+slope+ca, trainData,method = 'class')
dt.predict <- predict(dt.fit, testData, type = 'class')
dt.probability <- predict(dt.fit, testData, type = 'prob')
# save model for shiny app
saveRDS(dt.fit, "./HeartDiseaseCleveland/decisionTree.rds")
```

3. **Evaluate the model** - 

**ROC curve shows accuracy of this model is 82%**


```{r}
rpart.plot(dt.fit)
# In order to assess the performance of our model, we will delineate the ROC curve. ROC is also known as Receiver Optimistic Characteristics
# ROC curve shows sensitivity = 82% 
auc.dt = roc(testData$target, factor(dt.predict, ordered = TRUE), plot = TRUE, col = "blue")
auc.dt


```



### Artificial Neural Networks (ANN) Model
1. **Overview**


2. **Run the model against training data, predict target on test data**

```{r}
ann.fit =neuralnet(target~ age+sex+cp+trestbps+chol+fbs+restecg+thalach+exang+oldpeak+slope+ca,trainData,linear.output=FALSE)

# Save model for shiny app
saveRDS(ann.fit, "./HeartDiseaseCleveland/artificialNeuralNetwork.rds")

plot(ann.fit)
ann.predict=neuralnet::compute(ann.fit,testData)
annResult=ann.predict$net.result
#since predicted variable is probabilty and continous, convert to  0 or 1 factor
annResult=ifelse(annResult>0.5,1,0)
```

3. **Evaluate the model** - 

**ROC curve shows accuracy of this model is 82%**

```{r}
auc.ann = roc(testData$target, factor(annResult, ordered = TRUE), plot = TRUE, col = "blue")
auc.ann

```



### Gradient Boosting Model

1. **Overview**
This uses ensemble models like weak decision trees. These decision trees combine together to form a strong model of gradient boosting.

Gradient Boosting Machine (for Regression and Classification) is a forward learning ensemble method. The guiding heuristic is that good predictive results can be obtained through increasingly refined approximations. H2O’s GBM sequentially builds regression trees on all the features of the dataset in a fully distributed way - each tree is built in parallel.


2. **Run the model against training data, predict target on test data**

```{r include=FALSE}
system.time(
gbm.fit <- gbm(target~ age+sex+cp+trestbps+chol+fbs+restecg+thalach+exang+oldpeak+slope+ca
, distribution = "bernoulli"
, data =trainData
, n.trees = 1000
, interaction.depth = 6
, n.minobsinnode = 10
, shrinkage = 0.1
, bag.fraction = 0.5
, train.fraction = nrow(trainData) / (nrow(trainData) + nrow(testData))
)
)
# Save model for shin app
saveRDS(gbm.fit, "./HeartDiseaseCleveland/gradientBoosting.rds")

# Determine best iteration based on test data
gbm.iter = gbm.perf(gbm.fit, method = "test")
model.influence = relative.influence(gbm.fit, n.trees = gbm.iter, sort. = TRUE)

#Plot the gbm model
plot(gbm.fit)
# Plot and calculate AUC on test data
gbm.test = predict(gbm.fit, newdata = testData, n.trees = gbm.iter)

```

3. **Evaluate the model** - 

**ROC curve shows accuracy of this model is 86%**

```{r}
auc.gbm = roc(testData$target, gbm.test, plot = TRUE, col = "red")
print(auc.gbm)
auc.gbm
```

